# Logistic Regression with a Neural Network Mindset  

This repository implements **Logistic Regression** using a **Neural Network approach** to perform **binary classification** (cat vs. non-cat). The implementation is inspired by the **Deep Learning Specialization by Andrew Ng**.

---

## üöÄ Project Overview  

Logistic Regression is a fundamental machine learning algorithm used for binary classification. This implementation follows a **Neural Network mindset**, using:  
‚úÖ **Vectorized computations** for efficiency.  
‚úÖ **Gradient descent optimization** for learning.  
‚úÖ **Visualization of the cost function** across iterations.  
‚úÖ **Analysis of different learning rates** to improve performance.  

---

## üìÇ Repository Structure  

- **`L_R.ipynb`** ‚Üí Jupyter Notebook containing the full implementation, step-by-step explanations, and visualizations.  
- **`lr_utils.py`** ‚Üí Helper functions for loading and processing the dataset.  
- **`datasets/train_catvnoncat.h5`** ‚Üí Training dataset.  
- **`datasets/test_catvnoncat.h5`** ‚Üí Test dataset.  
- **`images`** ‚Üí For further analysis and testing the trained logistic regression with other images.
---

## üèóÔ∏è Model Architecture  

The logistic regression model follows these steps:  

1Ô∏è‚É£ **Initialize Parameters**: Set $W$ and $b$ to zero.  

2Ô∏è‚É£ **Forward Propagation**: Compute $Z = W^T X + b$, then apply the **sigmoid activation function**:  
   $\hat{Y} = \sigma(Z) = \frac{1}{1 + e^{-Z}}$ 

3Ô∏è‚É£ **Compute Cost**: Use **binary cross-entropy loss** to measure the difference between predictions and true labels:  
   $J = -\frac{1}{m} \sum \left( Y \log(\hat{Y}) + (1-Y) \log(1-\hat{Y}) \right)$  

4Ô∏è‚É£ **Backward Propagation**: Compute gradients \( dW \) and \( db \) to determine how to adjust parameters.  

5Ô∏è‚É£ **Update Parameters**: Apply **gradient descent** to optimize \( W \) and \( b \):  
   $W = W - \alpha dW, \quad b = b - \alpha db$  

6Ô∏è‚É£ **Make Predictions**: Convert probabilities into binary labels:
Make Predictions: Convert probabilities into binary labels: 
≈∑ =
{1,  if A ‚â• 0.5  
0,  if A < 0.5}


   ü§ù Contributing
Feel free to fork, submit issues, or contribute improvements! üöÄ
